{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "lastEditStatus": {
   "notebookId": "kaffc5suc274e4vqmu6l",
   "authorId": "158808794318",
   "authorName": "SIKHADAS",
   "authorEmail": "sikha.das@snowflake.com",
   "sessionId": "10695abe-1dd6-4bf4-8977-b5f97cb170b0",
   "lastEditTime": 1752782428559
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "430ebdf6-dab3-4096-aec8-26c9fe17f603",
   "metadata": {
    "collapsed": false,
    "name": "cell1"
   },
   "source": [
    "# :chart_with_upwards_trend: Telco Churn Model - Part 2 (Feature Engineering)\n",
    "\n",
    "### Please run the first Notebook fully before running this second Notebook.\n",
    "\n",
    "### First, add the `imbalanced-learn`, `snowflake-ml-python`, `altair`, `pandas`, and `numpy` packages from the package picker on the top right. We will be using these packages later in the notebook.\n",
    "\n",
    "To prepare our data for our model, we'll need to handle the imbalanced data problem by upsampling our dataset. \n",
    "\n",
    "For this, we'll be using the `SMOTE` algorithm from the `imblearn` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport streamlit as st\nimport altair as alt\nfrom imblearn.over_sampling import SMOTE \nfrom snowflake.ml.registry import Registry\nfrom snowflake.ml._internal.utils import identifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\nsession.query_tag = {\"origin\":\"sf_sit-is\", \n                     \"name\":\"churn_prediction\", \n                     \"version\":{\"major\":1, \"minor\":0},\n                     \"attributes\":{\"is_quickstart\":1, \"source\":\"notebook\"}}\n\n# Saving telco_churn_pdf into variable from Snowflake\ntelco_churn_pdf = session.sql(\"SELECT * FROM TELCO_CHURN_PDF\").to_pandas()\n\n# Extract the training features\nfeatures_names = [col for col in telco_churn_pdf.columns if col not in ['Churn']]\nfeatures = telco_churn_pdf[features_names]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3184b8-b0c2-49c5-a8f8-93ac84fcfd41",
   "metadata": {
    "name": "cell3",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# extract the target\n",
    "target = telco_churn_pdf['Churn']\n",
    "st.markdown(\"## Lets balance the dataset.\")\n",
    "# upsample the minority class in the dataset\n",
    "upsampler = SMOTE(random_state = 111)\n",
    "features, target = upsampler.fit_resample(features, target)\n",
    "st.dataframe(features.head())\n",
    "\n",
    "st.markdown(\"## Upsampled data.\")\n",
    "upsampled_data = pd.concat([features, target], axis=1)\n",
    "upsampled_data.reset_index(inplace=True)\n",
    "upsampled_data.rename(columns={'index': 'INDEX'}, inplace=True)\n",
    "st.dataframe(upsampled_data.head())\n",
    "\n",
    "st.markdown(\"## Preview of upsampled data.\")\n",
    "upsampled_data = session.create_dataframe(upsampled_data)\n",
    "# Get the list of column names from the dataset\n",
    "feature_names_input = [c for c in upsampled_data.columns if c != '\"Churn\"' and c != \"INDEX\"]\n",
    "upsampled_data[feature_names_input]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1f5f7f-777a-4f26-82cc-213b6b3c04e0",
   "metadata": {
    "collapsed": false,
    "name": "cell4"
   },
   "source": [
    "Once that's taken care of, we'll use scikit-learn to preprocess our data into a format that the model expects. This means scaling our features and splitting our data into training and testing datasets.\n",
    "\n",
    "We can perform StandardScaler preprocessing via sklearn to process in-memory or Snowpark ML preprocessing for pushdown compute.\n",
    "\n",
    "## Sci-kit learn Preprocessing with Pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9533336-f5d3-461c-aa75-b604e23e44c1",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": [
    "import sklearn.preprocessing as pp_original\n",
    "# Initialize a StandardScaler object with input and output column names\n",
    "scaler = pp_original.StandardScaler()\n",
    "features_pdf = upsampled_data[feature_names_input].to_pandas()\n",
    "\n",
    "# Fit the scaler to the dataset\n",
    "scaler.fit(features_pdf)\n",
    "\n",
    "# Transform the dataset using the fitted scaler\n",
    "scaled_features = scaler.transform(features_pdf)\n",
    "scaled_features = pd.DataFrame(scaled_features, columns = features_names)\n",
    "scaled_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a35603-423c-4da8-9e4c-cb9d77c2e081",
   "metadata": {
    "collapsed": false,
    "name": "cell6"
   },
   "source": [
    "## Snowpark ML preprocessing with Snowpark \n",
    "\n",
    "Note the similarity between the APIs used for sklearn and Snowpark ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa37e57b-9410-47d8-9d88-d3d3438f3469",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": [
    "import snowflake.ml.modeling.preprocessing as pp\n",
    "\n",
    "# Initialize a StandardScaler object with input and output column names\n",
    "scaler = pp.StandardScaler(\n",
    "    input_cols=feature_names_input,\n",
    "    output_cols=feature_names_input\n",
    ")\n",
    "\n",
    "# Fit the scaler to the dataset\n",
    "scaler.fit(upsampled_data)\n",
    "\n",
    "# Transform the dataset using the fitted scaler\n",
    "scaled_features = scaler.transform(upsampled_data)\n",
    "scaled_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410681cf-c3b1-47dd-9b58-85c2b2afbc8f",
   "metadata": {
    "collapsed": false,
    "name": "cell8"
   },
   "source": [
    "## Let's perform the train test split using 80/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589337dc-d674-40d6-9b73-a9d35fa28086",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": [
    "# Split the scaled_features dataset into training and testing sets with an 80/20 ratio\n",
    "training, testing = scaled_features.random_split(weights=[0.8, 0.2], seed=111)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd84cf31-4ae6-4f9b-a805-cad7f0d1c38e",
   "metadata": {
    "collapsed": false,
    "name": "cell10"
   },
   "source": [
    "# Model Training - Random Forest Classifier \n",
    "\n",
    "The mystery model of the day is a [random forest classifier](https://towardsdatascience.com/understanding-random-forest-58381e0602d2). I'll spare you the details on how it works, but in short, it creates an ensemble of smaller models that all make predictions on the same data. Whichever prediction has the most votes is the final prediction that the model goes with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15803558-604a-4312-b838-46c71ec74bf3",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": [
    "from snowflake.ml.modeling.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the target variable (label) column name\n",
    "label = ['\"Churn\"']\n",
    "\n",
    "# Define the output column name for the predicted label\n",
    "output_label = ['\"predicted_churn\"']\n",
    "\n",
    "# Initialize a RandomForestClassifier object with input, label, and output column names\n",
    "model = RandomForestClassifier(\n",
    "    input_cols=feature_names_input,\n",
    "    label_cols=label,\n",
    "    output_cols=output_label,\n",
    ")\n",
    "\n",
    "# Train the RandomForestClassifier model using the training set\n",
    "_ = model.fit(training)\n",
    "\n",
    "# Predict the target variable (churn) for the testing set using the trained model\n",
    "results = model.predict(testing)\n",
    "\n",
    "testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701cf188-e74f-4de6-8f56-1569879ac2d4",
   "metadata": {
    "collapsed": false,
    "name": "cell12"
   },
   "source": [
    "# Model Evaluation\n",
    "\n",
    "Model evaluation is all about checking how well our machine learning model is doing by comparing its predictions to the actual outcomes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc8f6c8-5d53-4608-a56d-e0e959a27bd0",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": [
    "# return only the predicted churn values\n",
    "predictions = results.to_pandas().sort_values(\"INDEX\")[output_label].astype(int).to_numpy().flatten()\n",
    "actual = testing.to_pandas().sort_values(\"INDEX\")[['Churn']].to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fd435b-b357-4d86-bc00-976884b9d590",
   "metadata": {
    "collapsed": false,
    "name": "cell14"
   },
   "source": [
    "## Feature Importance\n",
    "\n",
    "Feature importance is all about figuring out which input variables are the real MVPs when it comes to making predictions with our machine learning model. We'll find out which features are the most important by looking at how much they contribute to the model's overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418c65c8-c6f2-4a63-b8cd-9c139a395a11",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "cell15"
   },
   "outputs": [],
   "source": [
    "rf = model.to_sklearn()\n",
    "importances = pd.DataFrame(\n",
    "    list(zip(features.columns, rf.feature_importances_)),\n",
    "    columns=[\"feature\", \"importance\"],\n",
    ")\n",
    "\n",
    "bar_chart = alt.Chart(importances).mark_bar().encode(\n",
    "    x=\"importance:Q\",\n",
    "    y=alt.Y(\"feature:N\", sort=\"-x\")\n",
    ")\n",
    "st.altair_chart(bar_chart, use_container_width=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf8d5df-4175-4ad2-9e35-6390b7a23e94",
   "metadata": {
    "name": "cell16"
   },
   "source": [
    "## Predicting churn for a new user\n",
    "Using our trained random forest model, we can make predictions that tell us whether a new customer will churn or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c729da0c-7352-4d15-9cd5-9269f5860574",
   "metadata": {
    "name": "cell17",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "account_weeks = \"10\"\n",
    "data_usage = \"1.7\"\n",
    "mins_per_month = \"82\"\n",
    "daytime_calls = \"67\"\n",
    "customer_service_calls = \"4\"\n",
    "monthly_charge = \"37\"\n",
    "roam_mins = \"0\"\n",
    "overage_fee = \"9.5\"\n",
    "renewed_contract = \"true\"\n",
    "has_data_plan = \"true\"\n",
    "user_vector = np.array([\n",
    "    account_weeks,\n",
    "    1 if renewed_contract else 0,\n",
    "    1 if has_data_plan else 0,\n",
    "    data_usage,\n",
    "    customer_service_calls,\n",
    "    mins_per_month,\n",
    "    daytime_calls,\n",
    "    monthly_charge,\n",
    "    overage_fee,\n",
    "    roam_mins,\n",
    "]).reshape(1,-1)\n",
    "\n",
    "user_dataframe = pd.DataFrame(user_vector, columns=[f'\"{_}\"' for _ in features.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e32c56a-5acf-4bd8-8a4a-959dade14591",
   "metadata": {
    "name": "cell18"
   },
   "source": [
    "#### Input dataframe for new user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4234c64-cd43-45a5-8fb8-f0856449723c",
   "metadata": {
    "name": "cell19",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "st.markdown(\"#### New user\")\n",
    "user_dataframe\n",
    "user_vector = scaler.transform(user_dataframe)\n",
    "st.markdown(\"#### Churn prediction\")\n",
    "model.predict(user_vector)[['\"predicted_churn\"']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf29e547-b163-431e-b0c9-12b55a2bdaaf",
   "metadata": {
    "name": "cell20",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "st.markdown(\"#### Scaled dataframe for new user\")\n",
    "st.dataframe(user_vector)\n",
    "st.markdown(\"#### Prediction\")\n",
    "predicted_value = model.predict(user_vector)[['\"predicted_churn\"']].values.astype(int).flatten()\n",
    "user_probability = model.predict_proba(user_vector)\n",
    "probability_of_prediction = max(user_probability[user_probability.columns[-2:]].values[0]) * 100\n",
    "prediction = 'churn' if predicted_value == 1 else 'not churn'\n",
    "st.markdown(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc85b84-1a0c-4055-bd3c-03c40d542399",
   "metadata": {
    "name": "cell21",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "col1, col2 = st.columns(2)\n",
    "\n",
    "with col1: \n",
    "    account_weeks = st.slider(\"AccountWeeks\", int(features[\"AccountWeeks\"].min()) , int(features[\"AccountWeeks\"].max()))\n",
    "    data_usage = st.slider(\"DataUsage\", int(features[\"DataUsage\"].min()) , int(features[\"DataUsage\"].max()))\n",
    "    mins_per_month = st.slider(\"DayMins\", int(features[\"DayMins\"].min()) , int(features[\"DayMins\"].max()))\n",
    "    daytime_calls = st.slider(\"DayCalls\", int(features[\"DayCalls\"].min()) , int(features[\"DayCalls\"].max()))\n",
    "    renewed_contract =  st.selectbox(\"Renewed Contract?\",('true','false'))\n",
    "    \n",
    "with col2: \n",
    "    monthly_charge = st.slider(\"MonthlyCharge\", int(features[\"MonthlyCharge\"].min()) , int(features[\"MonthlyCharge\"].max()))\n",
    "    roam_mins = st.slider(\"RoamMins\", int(features[\"RoamMins\"].min()) , int(features[\"RoamMins\"].max()))\n",
    "    customer_service_calls = st.slider(\"CustServCalls\", int(features[\"CustServCalls\"].min()) , int(features[\"CustServCalls\"].max()))\n",
    "    overage_fee = st.slider(\"OverageFee\", int(features[\"OverageFee\"].min()) , int(features[\"OverageFee\"].max()))\n",
    "    has_data_plan = st.selectbox(\"Has Data Plan?\",('true','false'))\n",
    "\n",
    "user_vector = np.array([\n",
    "    account_weeks,\n",
    "    1 if renewed_contract else 0,\n",
    "    1 if has_data_plan else 0,\n",
    "    data_usage,\n",
    "    customer_service_calls,\n",
    "    mins_per_month,\n",
    "    daytime_calls,\n",
    "    monthly_charge,\n",
    "    overage_fee,\n",
    "    roam_mins,\n",
    "]).reshape(1,-1)\n",
    "\n",
    "user_dataframe = pd.DataFrame(user_vector, columns=[f'\"{_}\"' for _ in features.columns])\n",
    "user_vector = scaler.transform(user_dataframe)\n",
    "with col1: \n",
    "    st.markdown(\"#### Input dataframe for new user\")\n",
    "    st.dataframe(user_dataframe)\n",
    "with col2:\n",
    "    st.markdown(\"#### Scaled dataframe for new user\")\n",
    "    st.dataframe(user_vector)\n",
    "\n",
    "st.markdown(\"#### Prediction\")\n",
    "predicted_value = model.predict(user_vector)[['\"predicted_churn\"']].values.astype(int).flatten()\n",
    "user_probability = model.predict_proba(user_vector)\n",
    "probability_of_prediction = max(user_probability[user_probability.columns[-2:]].values[0]) * 100\n",
    "prediction = 'churn' if predicted_value == 1 else 'not churn'\n",
    "st.markdown(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adeb21c7-6dbd-4241-af24-414b3f8ec14a",
   "metadata": {
    "name": "cell22",
    "collapsed": false
   },
   "source": "## Logging Model to Snowflake Model Registry and Inference"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb791fe7-a127-4021-81f5-a76de0587abe",
   "metadata": {
    "name": "cell23",
    "language": "python"
   },
   "outputs": [],
   "source": "user_dataframe = pd.DataFrame(user_vector, columns=[f'\"{_}\"' for _ in features.columns])\nX = scaler.transform(user_dataframe)\n\ndb = identifier._get_unescaped_name(session.get_current_database())\nschema = identifier._get_unescaped_name(session.get_current_schema())\n\n# Define model name\nmodel_name = \"TELCO_EDA_MODEL\"\n\n# Create a registry and log the model\nnative_registry = Registry(session=session, database_name=db, schema_name=schema)\n\n# Let's first log the very first model we trained\nmodel_ver = native_registry.log_model(\n    model_name=model_name,\n    version_name='V0',\n    model=rf,\n    sample_input_data=X, # to provide the feature schema\n    target_platforms={'WAREHOUSE'},\n    options={\"enable_explainability\": False}\n)"
  },
  {
   "cell_type": "code",
   "id": "a4cf14a9-69cb-4006-b844-7a917ee55774",
   "metadata": {
    "language": "python",
    "name": "cell25"
   },
   "outputs": [],
   "source": "native_registry.get_model(model_name).show_versions() ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cb20515b-724f-4a09-8852-7e7849716afc",
   "metadata": {
    "language": "python",
    "name": "cell26"
   },
   "outputs": [],
   "source": "result_sdf = model_ver.run(X, function_name=\"predict\").values.astype(int).flatten()\nprediction = result_sdf[0]\n\nprediction = 'churn' if predicted_value == 1 else 'not churn'\nst.markdown(prediction)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e27a2d28-7c63-4590-a0cf-f4c091288a42",
   "metadata": {
    "language": "python",
    "name": "cell27"
   },
   "outputs": [],
   "source": "# Clean up\nnative_registry.delete_model(model_name)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "27b9081b-936a-4d1d-b6c9-c408e63e5da2",
   "metadata": {
    "language": "python",
    "name": "cell28",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "native_registry.show_models()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0fabfc3c-29eb-438e-8f5c-cf8afaf85e6e",
   "metadata": {
    "name": "cell24",
    "collapsed": false
   },
   "source": [
    "Congratulations on making it to the end of this Lab where we explored churn modeling using Snowflake Notebooks! We learned how to import/load data to Snowflake, train a Random Forest model, visualize predictions, and build an interactive data app, and make predictions for new users.\n"
   ]
  }
 ]
}